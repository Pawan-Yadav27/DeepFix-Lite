{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "getData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.0 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "efa960d91a83404a0e734f3d71ae38205449cca1b61855dfb07d190bd7f56433"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DeepFix : Deep learning for automatically fixing single-line syntax errors in C programs (using Encoder-Decoder model approach.) \r\n",
        "\r\n",
        "Import required libraries ---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "#from keras.layers import Input, LSTM, Dense\r\n",
        "from tensorflow.keras.models import Model \r\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense , Bidirectional ,  Concatenate\r\n",
        "from tensorflow.keras.preprocessing.text import one_hot\r\n",
        "from tensorflow.keras.layers import Embedding\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from operator import itemgetter\r\n",
        "import pandas as pd\r\n",
        "import pickle \r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import csv\r\n",
        "\r\n",
        "from customfunction import text2sequence , sequence2tokenization , encoder_decoder_data\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "gUmCzZJVlpAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data & Build vocabulary on traning data ---\r\n",
        "\r\n",
        "Dataset Description :\r\n",
        "\r\n",
        "Each row of the dataset is buggy C program line and 'target' C line which represents fixed C program line."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "data=pd.read_csv('G:/IISc/2nd-semester/ASE with ML/Assignment/DeepFixLite/train.csv')\r\n",
        "\r\n",
        "input_texts = data['sourceLineTokens']\r\n",
        "target_texts = data['targetLineTokens']\r\n",
        "\r\n",
        "input_lines , target_lines , token2count = text2sequence(input_texts , target_texts)"
      ],
      "outputs": [],
      "metadata": {
        "id": "r4dqoBxdqIQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)Setting parameters such as sentence length, vocabulory size = num_unique_tokens.\r\n",
        "\r\n",
        "(2) Select top 1000 most frequent words and construct vocabulary only for that many words. \r\n",
        "\r\n",
        "(3) Download Dictionary."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "k=1000 # select top 1000 words(most frequent)\r\n",
        "sent_length=70\r\n",
        "num_unique_tokens = k + 4\r\n",
        "num_samples = len(input_lines)\r\n",
        "\r\n",
        "token_with_count = dict(sorted(token2count.items(), key = itemgetter(1), reverse = True)[:k])\r\n",
        "token2index = {\"PAD\":0, \"SOS\":1, \"EOS\":2, \"OOV\":3}\r\n",
        "index2token = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"OOV\"}\r\n",
        "num_count = 4\r\n",
        "for i in token_with_count:\r\n",
        "  token2index[i] = num_count\r\n",
        "  index2token[num_count] = i\r\n",
        "  num_count += 1\r\n",
        "\r\n",
        "pickle_out1 = open(\"token2index.pickle\" , \"wb\")\r\n",
        "pickle_out2 = open(\"index2token.pickle\" , \"wb\")\r\n",
        "\r\n",
        "pickle.dump(token2index , pickle_out1)\r\n",
        "pickle.dump(index2token , pickle_out2)\r\n",
        "\r\n",
        "pickle_out1.close()\r\n",
        "pickle_out2.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "zt1YWj82q05-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "max_encoder_seq_length = max([len(eval(item)) for item in input_lines]) #154\r\n",
        "max_decoder_seq_length = max([len(eval(item)) for item in target_lines]) #169\r\n",
        "\r\n",
        "print(\"size of the original vocabulary =\",len(token2count))\r\n",
        "print(\"size of the updated vocabulary =\",len(token_with_count))\r\n",
        "print(\"max_encoder_seq_length =\",max_encoder_seq_length)\r\n",
        "print(\"max_decoder_seq_length =\",max_decoder_seq_length)\r\n",
        "print(\"number of samples =\",num_samples )\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of the original vocabulary = 5205\n",
            "size of the updated vocabulary = 1000\n",
            "max_encoder_seq_length = 154\n",
            "max_decoder_seq_length = 169\n",
            "number of samples = 14643\n"
          ]
        }
      ],
      "metadata": {
        "id": "SDfQCvXWrKSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating fixed size sequences which can be given input to neural networks\r\n",
        "\r\n",
        "(1) Tokenise input sequence \r\n",
        "\r\n",
        "(2) Add padding to make equal size sequence  \r\n",
        "\r\n",
        "Above two functionalities implemented in sequence2tokenization function. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "padded_input_texts = sequence2tokenization(input_lines , token2index , sent_length)\r\n",
        "padded_target_texts = sequence2tokenization(target_lines , token2index , sent_length)\r\n",
        "\r\n",
        "print(padded_input_texts[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1 21  4 82 48  8 42 11 17  5  2  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ],
      "metadata": {
        "id": "kM-u1d9stcQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Input data, decoder input data are inputs that need to be given to LSTM's networks for encoding and decoding."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "encoder_input_data , decoder_input_data , decoder_target_data = encoder_decoder_data(padded_input_texts , padded_target_texts ,num_samples ,sent_length ,num_unique_tokens )\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "ndeUKq3cteQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre_process the validation data(same step as we did previously on training data) for parameter tunning."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "valid_data=pd.read_csv('G:/IISc/2nd-semester/ASE with ML/Assignment/DeepFixLite/valid_complete.csv')\r\n",
        "\r\n",
        "input_valid_texts = valid_data['sourceLineTokens']\r\n",
        "target_valid_texts = valid_data['targetLineTokens']\r\n",
        "num_valid_samples = len(input_valid_texts)\r\n",
        "padded_input_valid_texts = sequence2tokenization(input_valid_texts , token2index , sent_length)\r\n",
        "padded_target_valid_texts = sequence2tokenization(target_valid_texts , token2index , sent_length)\r\n",
        "\r\n",
        "encoder_input_valid_data ,decoder_input_valid_data ,decoder_target_valid_data = encoder_decoder_data(padded_input_valid_texts , padded_target_valid_texts ,num_valid_samples ,sent_length ,num_unique_tokens )\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "vXgS5VQ_xM8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Encoder-decoder model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "latent_dim=256\r\n",
        "batch_size=64\r\n",
        "epochs = 10\r\n",
        "emb_size = 50\r\n",
        "\r\n",
        "\r\n",
        "#Training Encoder\r\n",
        "encoder_inputs = Input(shape=(None,))\r\n",
        "encoder_embedding=  Embedding(num_unique_tokens, emb_size)(encoder_inputs)\r\n",
        "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True))\r\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c =encoder_lstm(encoder_embedding)\r\n",
        "state_h = Concatenate()([forward_h, backward_h])\r\n",
        "state_c = Concatenate()([forward_c, backward_c])\r\n",
        "encoder_states = [state_h, state_c]\r\n",
        "\r\n",
        "#Training Decoder\r\n",
        "decoder_inputs = Input(shape=(None,))\r\n",
        "decoder_embedding=  Embedding(num_unique_tokens, emb_size)\r\n",
        "final_dex= decoder_embedding(decoder_inputs)\r\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\r\n",
        "decoder_outputs, _, _ = decoder_lstm(final_dex,initial_state=encoder_states)\r\n",
        "decoder_dense = Dense(num_unique_tokens, activation='softmax')\r\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\r\n",
        "\r\n",
        "# Define the model that will turn\r\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\r\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\n",
        "# Compile & run training\r\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 50)     50200       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 512), (None, 628736      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 50)     50200       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 512)          0           bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 512),  1153024     embedding_1[0][0]                \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 1004)   515052      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,397,212\n",
            "Trainable params: 2,397,212\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "1xCKRrNJ0vTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.fit(\r\n",
        "    [encoder_input_data, decoder_input_data],decoder_target_data,\r\n",
        "    batch_size=batch_size,\r\n",
        "    epochs=20,\r\n",
        "    #validation_split=0.2,\r\n",
        "    validation_data=([encoder_input_valid_data,decoder_input_valid_data],decoder_target_valid_data)\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "9wb4enPe06ro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#save the model\r\n",
        "tf.keras.models.save_model(model, \"G:/IISc/2nd-semester/ASE with ML/Assignment/DeepFixLite/BD_LSTM_model\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "1LM2TGUv1Ljh"
      }
    }
  ]
}
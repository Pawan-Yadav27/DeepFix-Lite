# -*- coding: utf-8 -*-
"""customFunction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CUNtnvjGz-nPHPb-CwjWe0vPwgYCEXCi
"""
from tensorflow.keras.preprocessing.sequence import pad_sequences
from operator import itemgetter
import tensorflow as tf
import numpy as np
import csv

def text2sequence(input_texts , target_texts):
  token2index = {"PAD":0 , "SOS":1 , "EOS":2}
  token2count = {}
  index2token = {0: "PAD", 1: "SOS", 2: "EOS"}
  num_tokens = 4
  input_lines=[]
  target_lines=[]

  for i in input_texts:
    #print(i)
    input_lines.append(i)
    s=eval(i)      
    for token in s:
      if token not in token2index:
        # First entry of token into vocabulary
        token2index[token] = num_tokens
        token2count[token] = 1
        index2token[num_tokens] = token
        num_tokens += 1
      else:
        # token exists; increase token count
        token2count[token] += 1
  
  for i in target_texts:
    target_lines.append(i)
    s=eval(i)          
    for token in s:
      if token not in token2index:
        # First entry of token into vocabulary
        token2index[token] = num_tokens
        token2count[token] = 1
        index2token[num_tokens] = token
        num_tokens += 1
      else:
        # token exists; increase token count
        token2count[token] += 1
  
  return input_lines , target_lines , token2count

def sequence2tokenization(texts , token2index , sent_length):
    indexed_texts =[]
    for i in texts:
      text=eval(i)
      temp_list=[]     
      for token in text:      
        if token in token2index.keys():
          temp_list.append(token2index[token])
        else:
          temp_list.append(3)
      indexed_texts.append(np.array([1]+temp_list+[2]))
      
    padded_seq = pad_sequences(indexed_texts,padding='post', maxlen = sent_length)
    return padded_seq

def encoder_decoder_data(padded_input_texts , padded_target_texts ,num_samples ,sent_length ,num_unique_tokens ):
  encoder_input_data = np.zeros((num_samples , sent_length),dtype='float32')
  decoder_input_data = np.zeros((num_samples, sent_length),dtype='float32')
  decoder_target_data = np.zeros((num_samples, sent_length,num_unique_tokens),dtype='float32')

  for i, (input_text, target_taxt) in enumerate(zip(padded_input_texts, padded_target_texts)):
    for t in range(len(input_text)):
      encoder_input_data[i, t] = input_text[t]
    
    for t in range(len(target_taxt)):
      decoder_input_data[i, t] = target_taxt[t]
 
      if t>0:
        decoder_target_data[i, t-1 , target_taxt[t]] = 1.0
  
  return encoder_input_data , decoder_input_data , decoder_target_data